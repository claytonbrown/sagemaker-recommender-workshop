{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie recommendation on Amazon SageMaker with Factorization Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "\n",
    "In many ways, recommender systems were a catalyst for the current popularity of machine learning.  One of Amazon's earliest successes was the \"Customers who bought this, also bought...\" feature, while the million dollar Netflix Prize spurred research, raised public awareness, and inspired numerous other data science competitions.\n",
    "\n",
    "Recommender systems can utilize a multitude of data sources and ML algorithms, and most combine various unsupervised, supervised, and reinforcement learning techniques into a holistic framework.  However, the core component is almost always a model which which predicts a user's rating (or purchase) for a certain item based on that user's historical ratings of similar items as well as the behavior of other similar users.\n",
    "\n",
    "Factorization Machines have shown themselves to be very effective at this task, and Amazon SageMaker includes an almost perfectly scalable implementation of a distributed Factorization Machine algorithm.  Today we'll use that to build a recommender system.  We'll start by bringing in and preparing our dataset, and then discuss the algorithm and training process, and finish by deploying to a real-time endpoint for generating predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Let's start by importing our necessary modules here, to get it out of the way.  Also, update the s3 bucket, prefix, or IAM role as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.predictor import json_deserializer\n",
    "\n",
    "import boto3, csv, io, json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defaults\n",
    "\n",
    "We will set a project name used to group Amazon SageMaker and Amazon S3 resources related to this specific notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'sagemaker-workshop-movie-recommender'\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.c5.18xlarge\"\n",
    "train_dataset_uri = 'http://files.grouplens.org/datasets/movielens/ml-latest.zip' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function: Identify SageMaker IAM service role\n",
    "\n",
    "SageMaker resources will need access to several resources to complete training jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sagemaker_role():\n",
    "    try:\n",
    "        # if running within Amazon SageMaker Notebook Instance, get current execution role\n",
    "        role = get_execution_role()\n",
    "    except:\n",
    "        # otherwise, guess the role\n",
    "        result = %sx aws iam list-roles | grep -e 'role/service-role' | grep -i 'sagemaker' | tr -d ' '\n",
    "        role = result[0][7:-2]\n",
    "        print(\"Found potential SageMaker service role: {}\".format(role))\n",
    "    return role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_sagemaker_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function: Identify SageMaker Container for specified region\n",
    "\n",
    "Set boto3 clients to use a valid SageMaker region and to select the proper container to use for training and endpoint deployment based on the specific algorithm to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_regions=['us-east-1','us-east-2','us-west-2','eu-west-1'] # available Amazon SageMaker regions\n",
    "region_name = boto3.Session().region_name\n",
    "if region_name not in available_regions:\n",
    "    region_name=available_regions[0]\n",
    "print('Setting SageMaker region to: {}'.format(region_name))\n",
    "default_bucket = '{}-{}'.format(project_name, region_name)\n",
    "print('Setting default S3 bucket to: {}'.format(default_bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: If not exists, create S3 bucket\n",
    "s3 = boto3.client(\"s3\", region_name=region_name)\n",
    "## check if bucket exists\n",
    "## if not...\n",
    "s3.create_bucket(Bucket=default_bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and look at our data\n",
    "Let's get our data. Factorization Machines work well as recommendation solutions, such as movie recommendations! Also, FM's do great on lots of data, so let's get something heavy. Maybe the MovieLens dataset, with 20 million ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f /tmp/dataset.zip\n",
    "!rm -rf /tmp/dataset\n",
    "!wget -O /tmp/dataset.zip $train_dataset_uri\n",
    "!unzip -j -o /tmp/dataset.zip -d /tmp/dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the top of this file, shall we? This should show us the range of user ids, which we will need later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /tmp/dataset\n",
    "!head -5000000 ratings.csv > ratings.trunc.csv\n",
    "!mv ratings.trunc.csv ratings.csv\n",
    "!head -10 ratings.csv\n",
    "!tail -10 ratings.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/tmp/dataset/ratings.csv\", sep=',')\n",
    "data = data.drop('timestamp',1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratings: User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ratings = data['userId'].value_counts()\n",
    "user_count = user_ratings.count()\n",
    "print(\"Users: {}\\n\".format(user_count))\n",
    "print(\"Users Most Rating: \\n{}\".format(user_ratings.head(n=10)))\n",
    "ax = user_ratings.hist(bins=250)\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ratings: Movie Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings = data['movieId'].value_counts()\n",
    "movie_count = movie_ratings.count()\n",
    "print(\"Movies Rated: {}\\n\".format(movie_count))\n",
    "print(\"Movies Most Rated: \\n{}\".format(movie_ratings.head(n=10)))\n",
    "ax = movie_ratings.hist(bins=250)\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomizing data\n",
    "\n",
    "We will now shuffle the data, splitting the dataset into training+validation(90%) and testing/prediction(10%)\n",
    "\n",
    "We will reshape the training+validation data for use by the Factorization Machine training, and set aside the remaining testing/prediction data for use when the endpoint is set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_training, data_prediction = train_test_split(data,test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training + validation set\n",
    "\n",
    "When using Factorization Machines for building a recommender system, the algorithm expects the data to look something like:\n",
    "\n",
    "|Rating|User1|User2|...|UserN|Movie1|Movie2|Movie3|...|MovieM|\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "|4|1|0|...|0|1|0|0|...|0|\n",
    "|5|1|0|...|0|0|1|0|...|0|\n",
    "|3|0|1|...|0|1|0|0|...|0|\n",
    "|4|0|1|...|0|0|0|1|...|0|\n",
    "\n",
    "- Our target column is the rating for that user movie combination.  This could be the number of stars given, or could be a binarized version (movies with ratings 4 and above are set to `1`, otherwise `0`).  \n",
    "- We have a set of `N` features which are a one-hot encoding of user.  They only take a value of `1` to indicate observations which belong to that customer.\n",
    "- We also have a set of `M` features which are a one-hot encoding of movie.\n",
    "\n",
    "Sagemaker's Factorization Machines can handle sparse data, meaning we only store the non-zero entries, but to contruct the sparse feature matrices we need to figure out the size of the training and testing sets. We also normalized the movie ids here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "feature_count = user_count+movie_count\n",
    "instance_count = data['userId'].count()\n",
    "\n",
    "data_onehot=pd.get_dummies(data_training.to_sparse(),columns=['userId','movieId'],sparse=True).astype('float32')\n",
    "# ~4MM instances, ml.m4.16xlarge\n",
    "# CPU times: user 56.7 s, sys: 828 ms, total: 57.6 s\n",
    "# Wall time: 57.3 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_onehot.head() # should see userId_1 filled in for initial entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_onehot.tail() # should be last column userId_<last_user_id> filled in for initial entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset -> features vs labels\n",
    "\n",
    "The data is currently held in a sparse matrix, we'll inspect it to see what it is all about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_onehot.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to split the prediction targets(y), ie. a vector of the ratings themselves, from the matrix of other predictive features(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "X = data_onehot.loc[:, 'userId_1'::]\n",
    "y = data_onehot.loc[:,'rating']\n",
    "\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversion\n",
    "Our final bit of data conversion is to convert the features of X and the labels of y into a RecordIO-protobuf format which streamlines the sparse data to be used for training.\n",
    "\n",
    "There are two ways to achieve this result:\n",
    "    - use a special library function to automatically convert the training/validation data to RecordIO protobuf format, and automatically upload those files to an S3 location\n",
    "    - use a custom function to index through the sparse dataset, writing the output buffer to a file and shipping that to a predetermined S3 bucket location\n",
    "\n",
    "For more information, see: [Training Data Serialization](https://alpha-docs-aws.amazon.com/sagemaker/latest/dg/cdf-training.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Automatic conversion of training/validation datasets and upload to S3\n",
    "\n",
    "This is a simpler method that works for smaller datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "max_records = 1000\n",
    "record_data_train = fm.record_set(X[0:max_records].values,labels=y[0:max_records],channel='train')\n",
    "\n",
    "\n",
    "# 1000 instances, notebook ml.m4.16xlarge\n",
    "# CPU times: user 28.3 s, sys: 1.05 s, total: 29.4 s\n",
    "# Wall time: 34.2 s\n",
    "    \n",
    "# 10000 instances, notebook ml.m4.16xlarge\n",
    "# CPU times: user 2min 31s, sys: 10.3 s, total: 2min 41s\n",
    "# Wall time: 3min 19s\n",
    "    \n",
    "# 50000 instances, notebook ml.m4.16xlarge\n",
    "# ClientError: An error occurred (EntityTooLarge) when calling the PutObject operation: Your proposed upload exceeds the maximum allowed size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Convert to protobuf and save to S3\n",
    "We train our models from data in S3; writing a helper function here to help us with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: use for larger datasets\n",
    "def writeDatasetToProtobuf(X, y, bucket, prefix, key):\n",
    "    from scipy.sparse import issparse\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, y)\n",
    "    buf.seek(0)\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    boto3.resource('s3').Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return 's3://{}/{}'.format(bucket,obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_data_location = writeDatasetToProtobuf(X.to_coo(), y, default_bucket, 'protobuf', 'train')    \n",
    "#test_data_location  = writeDatasetToProtobuf(X_test, Y_test, default_bucket, test_prefix, test_key)\n",
    "\n",
    "#train_data_location = \"s3://{}/{}/train/{}\".format(default_bucket, prefix, train_key)  \n",
    "#test_data_location  = \"s3://{}/{}/test/{}\".format(default_bucket, prefix, test_key)\n",
    "  \n",
    "print(train_data_location)\n",
    "\n",
    "# ~5MM instances, ml.m4.16xlarge\n",
    "# CPU times: user 9min 51s, sys: 7.6 s, total: 9min 59s\n",
    "# Wall time: 9min 43s\n",
    "\n",
    "# ~4.5MM instances, ml.m4.16xlarge, training dataset only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Factorization Machine training job with validation\n",
    "Sagemaker algorithms are stored as Docker containers in ECS, and we need the URI of the containers. This will be region specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_containers = {\n",
    "    'us-west-2': '174872318107.dkr.ecr.us-west-2.amazonaws.com/factorization-machines:latest',\n",
    "    'us-east-1': '382416733822.dkr.ecr.us-east-1.amazonaws.com/factorization-machines:latest',\n",
    "    'us-east-2': '404615174143.dkr.ecr.us-east-2.amazonaws.com/factorization-machines:latest',\n",
    "    'eu-west-1': '438346466558.dkr.ecr.eu-west-1.amazonaws.com/factorization-machines:latest'}\n",
    "\n",
    "training_image = fm_containers[region_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model on SageMaker training instances\n",
    "\n",
    "Training time! We will now fit the model using SageMaker instance(s), this could take some time.\n",
    "\n",
    "Some things to note here that might not be obvious:\n",
    "<ul>\n",
    "    <li>You can specify the hardware you want to use; Sagemaker will create your cluster and deploy your image</li>\n",
    "    <li>You can optionally proviode a test dataset as well, which we have done here</li>\n",
    "    <li>The metrics will be printed out after each epoch and mini batch</li>\n",
    "</ul>\n",
    "\n",
    "In order to train our Factorization Machine, we'll need to specify some hyperparameters.  First, let's think about what a Factorization Machine does:  \n",
    "\n",
    "$$\\hat{r} = w_0 + \\sum_{i} {w_i x_i} + \\sum_{i} {\\sum_{j > i} {\\langle v_i, v_j \\rangle x_i x_j}}$$\n",
    "\n",
    "where $\\hat{r}$ is the movie rating, $x_i$ are the one-hot encoded user and movie indicators, $w_i$ are linear weights, and the second term represents the pairwise feature interactions as a reduced dimension factorization.  This reduction in dimensionality allows us to find a smaller, more generalizable representation of information in the large sparse input dataset.  Key hyperparameters are therefore:\n",
    "\n",
    "- `feature_dim`: How big our initial movie \n",
    "- `num_factors`: How large our reduced dimensionality representation of interactions should be\n",
    "\n",
    "We also specify hyperparameters like `mini_batch_size` and `epochs` which can be tuned for improved performance.\n",
    "\n",
    "Hit \"play\" and sit back to let Sagemaker do all the work here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker import FactorizationMachines as FM\n",
    "\n",
    "model_output_path = 's3://{}/{}/output'.format(default_bucket, \"fm\")\n",
    "fm = FM(role=role,\n",
    "        train_instance_count=train_instance_count,\n",
    "        train_instance_type=train_instance_type,\n",
    "        output_path=model_output_path,\n",
    "        num_factors=64,\n",
    "        epochs=3,\n",
    "        predictor_type='regressor')\n",
    "\n",
    "fm.fit(record_data_train, mini_batch_size=1000)\n",
    "\n",
    "# 1000 instances, notebook ml.m4.16xlarge, training instance 1*ml.c5.18xlarge, training dataset only\n",
    "# Billable seconds: 106\n",
    "# CPU times: user 38.7 s, sys: 956 ms, total: 39.7 s\n",
    "# Wall time: 4min 51s\n",
    "\n",
    "# 10000 instances, notebook ml.m4.16xlarge, training instance 1*ml.c5.18xlarge, training dataset only\n",
    "# Billable seconds: 181\n",
    "# CPU times: user 2min 44s, sys: 8.19 s, total: 2min 52s\n",
    "# Wall time: 8min 41s\n",
    "# 50000 instances, notebook ml.m4.16xlarge, training instance 1*ml.c5.18xlarge, training dataset only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy model\n",
    "Now our trained model is stored, and you can actually see this in the Sagmaker console. We can go ahead and deploy this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fm_predictor = fm.deploy(instance_type='ml.c5.2xlarge', initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to make requests to our endpoint for prediction. Our data is really sparse (only 2 values in each vector row of 165237), and it doesn't make sense to send matrices that are mostly empty. To solve this problem, we are going to go ahead and create a protobuf serializer to send content in a sparse format is supported by sagemaker.\n",
    "\n",
    "You can see more formats here: https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-inference.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_predictor.content_type = 'application/x-recordio-protobuf'\n",
    "fm_predictor.serializer = sparse_protobuf_serializer\n",
    "fm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions\n",
    "We made a dataset to run prediction earlier in this notebook. Let's go ahead and load that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred, Y_pred = loadDataset('ratings.shuffled.prediction', lines_prediction , nbFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab single row and run prediction on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_result = fm_predictor.predict(X_pred[:1])\n",
    "print(fm_result)\n",
    "true_labels = Y_pred[:1]\n",
    "\n",
    "for l,r in zip(true_labels, fm_result['predictions']):\n",
    "    print(\"TrueLabel:{} PredictedLabel:{} Score:{}\".format(l, int(r['predicted_label']), r['score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of training job's accuracy\n",
    "Now that we've sanity checked that, let's just run inference on the entire prediction dataset of 2 million rows. Our feature dimension is wide but our data is mostly sparse, so we should be able to do relatively big batches of inferences. (This will take a couple of minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "batch_size = lines_prediction\n",
    "predictions = []\n",
    "\n",
    "batches = int(X_pred.shape[0]/batch_size)\n",
    "\n",
    "for i in range(batches):\n",
    "    X_sample = X_pred[ (i*batch_size) : ((i+1)*batch_size) ]\n",
    "    result = fm_predictor.predict(X_sample)\n",
    "    predictions += [r['predicted_label'] for r in result['predictions']]\n",
    "\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_result = fm_predictor.predict(X_pred[1000:1003])\n",
    "print(fm_result)\n",
    "print(Y_pred[1000:1003])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what percentage we recommended correctly. This should be roughly equivalent to our testing metric that was reported during training, but provides independent validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0]\n",
    "np.sum(Y_pred == predictions) / len(Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a substantial improvement over a naive baseline ... add more here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up, and finish!\n",
    "And we are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
